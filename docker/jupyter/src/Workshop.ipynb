{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling Machine Learning Pipelines using Apache Airflow\n",
    "\n",
    "In this workshop, you will use Airflow to schedule a basic machine learning pipeline. The workshop consists of 3 assignments.\n",
    "\n",
    "1. Schedule a basic 'hello world' example on Airflow\n",
    "2. Schedule a machine learning pipeline on Airflow\n",
    "3. Improve the the pipeline by creating your own custom Airflow operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Hello World\n",
    "\n",
    "In this assignment, we are going to schedule a simple workflow on Airflow to get used with the concepts. The code below defines a DAG (directed acyclic graph) in Airflow. Inspect it and learn what the individual components mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# defines default arguments, which are added to every operator of a DAG (if not overriden)\n",
    "default_args = {\n",
    "    'start_date': datetime.datetime(2019, 10, 1)\n",
    "}\n",
    "\n",
    "# defines a DAG\n",
    "# Any operator initialized inside this context manager will be added to the DAG\n",
    "with DAG(\n",
    "    dag_id='hello_world', # Each DAG has a unique ID\n",
    "    schedule_interval='0 0 * * *', # Each DAG has a schedule defnied using the Cron language. This DAG will run once a day at midnight.\n",
    "    default_args=default_args\n",
    "):\n",
    "    # Defnies a Python operator, which can execute any python callable\n",
    "    print_hello_operator = PythonOperator(\n",
    "        task_id='print_hello', # Every task within a DAG should have a unique ID\n",
    "        python_callable=print, # The python callable that is used by the operator\n",
    "        op_args=['hello'] # The arguments passed to the python callable\n",
    "    )\n",
    "    \n",
    "    def print_world(): \n",
    "        print('world')\n",
    "\n",
    "    print_world_operator = PythonOperator(\n",
    "        task_id='print_world',\n",
    "        python_callable=print_world, # In this operator, we used our own custom function\n",
    "    )\n",
    "\n",
    "    # Airflow offers many different types of operators. \n",
    "    # In this task, we use a Bash operator to, again, print something.\n",
    "    print_airflow_operator = BashOperator(\n",
    "        task_id='print_airflow',\n",
    "        bash_command='echo airflow'\n",
    "    )\n",
    "\n",
    "    # The bitshift operator '>>' is overloaded by Airflow.\n",
    "    # It is used to define the dependencies between tasks in a DAG.\n",
    "    # In this case, print_world will run when print_hello is finished.\n",
    "    # print_airflow will run as well when print_hello is finished.\n",
    "    # Airflow could be configured to run these tasks in parallel, as they do not need to wait for each other.\n",
    "    print_hello_operator >> print_world_operator\n",
    "    print_hello_operator >> print_airflow_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything we need to do to define a DAG that can be scheduled on Airflow. Because DAGs are defined using Python, we have a lot of freedom in how we want to design our DAG. We could, for example, dynamically create tasks by looping over lists. Furthermore, defining your DAGs as code makes it it easy to keep track on their version in a source code management system. \n",
    "\n",
    "The next step is to actually run this example on Airflow. The Airflow scheduler periodically scans a folder, dubbed 'the DagBag', for files that define DAGs. There is a folder in this jupyter notebook server, called `dags`, which is also present at the airflow scheduler via a network file system. Any python file that we put there, will be picked up by the scheduler.\n",
    "\n",
    "- Copy the code snippet above. Go to the dags folder in the file explorer, press the 'New' button in the upper right corner, and create a **Text File**. Name it `hello_world.py`. The name of the file is flexible, but should end with `.py`. Paste the copied code inside.\n",
    "- Go to port 8080 of your personal load balancer (you are now on 8888). Your DAG should appear within a few minutes.\n",
    "- The DAG is turned off at first. Turn it on and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Machine Learning Pipeline\n",
    "\n",
    "In this assignment, we will create an Airflow DAG to schedule a basic machine learning pipeline. The pipeline will use the famous Iris dataset and consists of 2 steps:\n",
    "\n",
    "1. Preprocess the dataset by adding some new features\n",
    "2. Train a predictive model on the dataset\n",
    "\n",
    "The goal is to schedule this training pipeline on a regular interval. This makes sure your model gets updated frequently with the latest data. In this example, we will use the same dataset for every run, but in reality you would like to use a new dataset every time the pipeline is run. This can be done using Airflow's [templating](https://airflow.apache.org/macros.html) mechanism, but is beyond the scope of this workshop.\n",
    "\n",
    "You are provided with 2 scripts located in the folder `transform_scripts`, called. Each script transforms an input file and stores it in an output file. The locations of the input and output files are provided as arguments to the scripts. The first script, `preprocess.py`, takes a CSV with raw training data, and outputs a CSV with preprocessed training data. The second script, `train.py`, takes a CSV with preprocessed training data, and outputs a pickled machine learning model. Also, you are provided with an S3 bucket containing our raw training data. Our DAG should to the following:\n",
    "\n",
    "1. Retrieve the raw training data from S3, apply the `preprocess.py` transform script to it, and send the preprocessed CSV back to S3\n",
    "2. Retrieve the preprocessed training data from S3, apply the `train.py` transform script to it, and send the pickled model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
